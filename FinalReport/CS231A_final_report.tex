\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins


\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{algpseudocode}
\usepackage[autolinebreaks,useliterate,framed]{mcode}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage{cite}
\usepackage{fancyhdr}
\usepackage{hyperref}
\newcommand{\op}[1]{\operatorname #1}
\newcommand{\eq}[1]{$\left(\ref{eq:#1}\right)$}
\newcommand{\fig}[1]{Fig. \ref{fig:#1}}
\newcommand{\pre}[2]{{}#1 #2}
\newcommand{\V}[1]{{\bf #1}}

\newcommand{\vect}[1]{\textbf{#1}}
\newcommand{\del}{\nabla}
\newcommand{\real}[1]{\mathbb{R}^{#1}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\step}[2]{{#1}^{\left({#2}\right)}}
\newcommand{\pinv}[1]{{#1}^\dagger}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{ {\bf Using Computer Vision for Real-time Estimation of Sail Angle for Performance Analysis and Automated Education } \\ CS231A Final Report}
\author{Andrew P. Kessler \\
\texttt{apkess@stanford.edu}}
\date{\today}                                      

\pagestyle{plain}
\pagenumbering{arabic}

\begin{document}
\maketitle


\begin{abstract}

This paper presents a real-time method for estimating the angle of a sailboat's boom relative to the boat centerline, based only on data gathered from a single on-board camera.  We outline the algorithm used, which relies on known geometry of the boat to enable 3D reconstruction from images. This feature could be useful for logging sail behavior for offline review, and could later be incorporated with wind-direction data for real-time suggestion of sail adjustments - a first step towards implementing a closed-loop sail angle controller. Results from experiments at full scale were promising, yielding estimations of sail angle accurate to within $\pm 10 ^\circ$. 
\end{abstract}

\section{Introduction}
\subsection{Problem Description}
Manning the helm of a sailboat of any size is a non-trivial skill; unlike driving a car or riding a bicycle, steering a wind-powered vessel demands an acute awareness of current wind conditions, the hydrodynamic properties of the boat, and an understanding of a complex series of mechanical linkages. 

Perhaps the most basic skill involved is optimizing the heading of the boat and the position of the sail(s) (jointly referred to as the ``point-of-sail") given the current wind direction and magnitude. In fact, this optimization can in theory be estimated quite accurately without access to a detailed model of the rest of the ship. Figure \ref{fig:pos} illustrates some basic points-of-sail; notice how the angle of the sails relative to the boat changes depending on the heading of the boat relative to the wind. 

\begin{figure}[htbp]
   \centering
   \includegraphics[width=.42\textwidth]{pos_annotated} % requires the graphicx package
   \caption{Some basic points-of-sail for a two-sailed vessel. Note how the angle of the sails relative to the boat ($\phi$) changes as the heading of the boat ($\theta$) changes. Adapted from \cite{pos_img}. }
   \label{fig:pos}
\end{figure}
For a given boat heading relative to the wind, there is an optimal angle at which the sails should be deployed relative to the boat, which could be modeled using some simple physics and trigonometry. Even if the wind direction is known exactly (which is rarely the case), determining this optimal angle in real-time can often prove difficult, even for the seasoned-sailor.

Additionally, having access to logged data of how the sails were deployed at a given time for offline review would be a very helpful tool for training new sailors. 

This project proposes the development a computer-vision based system that would would use data from an on-board camera to estimate the current angle of the sails based on known geometric constraints of the boat, and log this data for later review. Additionally, in real-time, the current sail angle can be compared to the optimal sail angle (based on current wind conditions) to provide a suggest sail adjustment (or \emph{trim}) to the user. 

\subsection{Proposed Solution}
The first step in tackling the problem is narrowing the scope to only consider a single type of boat/sail-configuration. This system presented was developed and tested using a \emph{pram}, which at eight-feet in length is a relatively small, flat-bottomed boat with a flat front (see Fig.~\ref{fig:pram}).

The base of the sail is firmly mounted to the \emph{boom}, which is essentially a long pole extending horizontally from the \emph{mast} (the vertical pole around which the boom rotates). The boom can swing anywhere within a $\pm 90^\circ$ range from the centerline of the boat, and is the main factor in determining the angle of the sail. For the purposes of this analysis, we will assume that the position boom uniquely determines the sail angle, although in reality, there are many other degrees of freedom that influence the sail angle and shape.

 The pilot controls boom angle via the \emph{mainsheet}: a line\footnote{Ropes in a maritime context are referred to as ``lines".} running from about three-quarters of the way down the boom to the center of the cockpit. By mounting a known and easily detectable fiducial at the endpoint of the boom (known as the \emph{clew}), the computer-vision system would segment the image based on color to extract the fiducial marker, and then use the proposed reconstruction algorithm to determine the angle of the boom.
 
\begin{figure}[htbp]
   \centering
   \includegraphics[width=.42\textwidth]{pram_alpha} 
   \caption{A representation of the Eastport Pram; a simple sailboat. The yellow dot is where the boom fiducial is mounted, $\ell_B$ is the length of the boom, and $\alpha$ is the boom angle. Adapted from \cite{clc:pram}.}
   \label{fig:pram}
\end{figure}

Additionally, the system could utilize outward facing cameras to look for objects that are able to hint at the current wind direction, and extract an estimate of wind direction from their configuration. Ideally, these would be wind socks and vanes. For the purposes of this proof of concept project, we assume that the current wind direction and heading is known \emph{a priori}.

With this information, the system can calculate the optimal boom angle. This could then be compared to the current boom angle, and a recommended adjustment would be suggested to the pilot. In fact, given adequate resolution and computational resources, this could even be used to implement a closed-loop ``cruise-control". (Note that this is only one step of steering a boat; many other controls would need to be automated before a true autonomous sailboat was possible - hence the cruise-control analogy.)

As mentioned above, using the boom angle as an approximation of the sail angle is a simplified model. Other factors, such as sail shape, can have a large impact on boat performance.

The system is implemented using the OpenCV package in C++ along with MATLAB for offline processing. A relatively cheap webcam was used for image acquisition. Experiments were performed both on a scale model designed specifically for this project, and on a preexisting full-size sailboat. 

\section{Literature Review}
\subsection{Previous Work}
Little work has been done to apply on-board computer vision to sailboats. There has been work inferring the shape of sails and other deformable surfaces without any \emph{a priori} knowledge of the surface, based only on 2D images \cite{Pilet:2008}. Since the sail shape gives information about how well the boat is catching wind, such data could later be incorporated into the method presented here to augment optimal sail angle decision making. 
 
However, several attempts at autonomous sailboats have been made over the past few years. One group has launched a fully autonomous sailboat, although the boat was designed from the beginning to amenable to robotic control \cite{Stelzer:2011},\cite{Sauz:2013}. As such, all of the feedback of current sail conditions are mechanical, and require elaborate hardware integrated into the boat. 
\subsection{Why Is This Method Better}
The method proposed here is a relatively non-invasive way to introduce automatic control to an existing sailboat. Other solutions (such as those proposed in \cite{Stelzer:2011}) require major modifications to the vessel. Here, only the mounting of a camera to the boat is necessary; no major physical modifications to the boat are required. In fact, several options for mounting cameras to a sailboat already exist (such as  \url{http://www.horizontrue.com/}), and would not need major modifications to work with this proposal.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Implementation}
The following sections describe the technical details of the method proposed. We begin in Section \ref{sec:calibration} by looking at the camera calibration process. Section \ref{sec:fid} explains how to extract the position of the boom fiducial in an acquired image. Then, Section \ref{sec:3d} proposes a method for reconstructing the boom angle from the fiducial position in an image. Finally, \ref{sec:optimal} discusses how to suggest sail adjustments given the current sail angle and how to calculate the optimal sail angle given the heading.
%
%
%%%%%%%%%%%%%%%%%%%%% CAMERA CALIBRATION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Camera Calibration}
\label{sec:calibration}
\subsubsection{Calibration Summary}
In order to do 3D reconstruction in later steps, we need to calibrate the camera for this scene. We assume that the camera is mounted securely to the back boat (facing forward), and that the boat geometry is known. To find the lumped camera matrix $M = K[R~t]$, we will observe the mappings of several calibration points of which 3D positions are known, and use a least-squares approach to solve for $M$. 
\subsubsection{Calibration Details}
If we assume we have an affine camera, every 3D point in the world frame has a corresponding 2D mapping in the image frame, and they have the relationship
\[
\left[\begin{array}{c}u_i \\v_i \\1\end{array}\right] =  
\left[\begin{array}{cccc}
m_{11} & m_{12} & m_{13} & m_{14} \\
m_{21} & m_{22} & m_{23} & m_{24} \\
0 & 0 & 0 & 1\end{array}\right]
\left[\begin{array}{c}X_i\\Y_i\\Z_i\\1\end{array}\right],
\]
where $(u,v)$ are the image coordinates and $(X,Y,Z)$ are the world coordinates.
This matrix equation can be expanded to
\begin{align*}
u_i &= m_{11}X_i + m_{12} Y_i + m_{13}Z_i + m_{14}\\
v_i &= m_{21}X_i + m_{22} Y_i + m_{23}Z_i + m_{24}.
\end{align*}
We can now factor out the unknowns $m_{jk}$ to get
\[
\left[\begin{array}{c}u_i \\v_i\end{array}\right] =  
\left[\begin{array}{cccccccc}
X_i & Y_i & Z_i & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & X_i & Y_i & Z_i & 1 \end{array}\right]
m,
\]
where 
\[m = \left[\begin{array}{cccccccc}
m_{11}& m_{12}& m_{13}& m_{14}& m_{21}&m_{22}&m_{23}& m_{24}
\end{array}\right]^T\]
 represents the unknown parameters of the calibration matrix.
Now, we can just stack the corresponding coordinates for each of $N$ calibration points, to get
\[
\left[\begin{array}{c}u_1 \\v_1 \\\vdots \\ u_N \\ v_N\end{array}\right]=
\left[\begin{array}{cccccccc}
X_1 & Y_1 & Z_1 & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & X_1 & Y_1 & Z_1 & 1 \\
\vdots  & \vdots  & \vdots  & \vdots   & \vdots &\vdots &\vdots &\vdots \\
X_N & Y_N & Z_N & 1 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & X_N & Y_N & Z_N & 1 \\ \end{array}\right]m
\]
which can be written more compactly as 
\[
c = Bm
\]
where $c \in \real{2N \times 1}$ is a vector of feature points in the images, $B \in \real{2N \times 8}$ is a matrix composed of the world coordinates of the features, and $m \in \real{8 \times 1}$ is a vector with the unknown parameters of the calibration.

Because $B$ is skinny [and assumed] full-rank, one way to minimize the mean-squared error of the world-points reprojected back into the image
\[
E = \norm{Bm - c}
\] is to just calculate the least-squares solution to $m$:
\[
m_{\text{ls}} = \left(B^TB\right)^{-1}B^Tc.
\]
Now we can just unpack $m$ to get $M$. The calibration points that were used for this project are  shown in Fig. \ref{fig:calib_labels}.
\begin{figure}[htbp]
   \centering
   \includegraphics[width=.47\textwidth]{calib_labels}
   \caption{The calibrations points used - world coordinates are known and labeled. Also, the world coordinate frame and origin is shown in black.}
   \label{fig:calib_labels}
\end{figure}

Now we know $p = MP$, where $p \in \real{3\times 1}$ is the image coordinate in homogenous coordinates, and $P \in \real{4 \times 1}$ is the corresponding world coordinate in homogenous coordinates. 
\subsection{Fiducial Detection}
\label{sec:fid}
\subsubsection{Detection Summary}
In this section, we explain how to take a raw image, and extract the 2D position of the fiducial marker (representing the end of the boom) from the image. We accomplish this using color thresholding and morphological operations. We assume that we know the color of the fiducial. 
\subsubsection{Detection Details}
We begin by taking a frame, and thresholding it in Hue-Saturation-Value (HSV) space to isolate a particular color fiducial. We use HSV thresholding as opposed to RGB thresholding because the HSV color basis is more robust to different lighting conditions and other factors \cite{hounslow:13}. 

We can then create a binary mask of the image, where each pixel is either zero or one, depending on whether or not the HSV value of that pixel in the source image was between the allowable minimum and maximum cutoffs for all of hue, value, and saturation. 
Assuming a proper calibration for appropriate threshold values, we are left with a black and white image, where points that match the desired color are white, and all others are black (see Fig. \ref{fig:premorph} for an example). 

Unfortunately, the thresholded image is very noisy; there are many points that passed through the color filter based on pure chance that do not correspond to our fiducial. We can attempt to remove some of this noise by applying morphological operations to the image. Morphological operations are filters that adjust an image based on the shape of its contents \cite{OpenCV:morph}. To remove extraneous white space, we can use the \emph{erosion} operator, which is done by sliding a small rectangular area over the image, and replacing each pixel with the minimum of the values that appear within the rectangle. 

\begin{figure}[h]
        \centering
         \begin{subfigure}[b]{0.2\textwidth}
                \includegraphics[width=\textwidth]{premorph.png}
                \caption{No morphological operations - raw result from HSV thresholding.}
                \label{fig:premorph}
        \end{subfigure}
        ~
        \begin{subfigure}[b]{0.2\textwidth}
                \includegraphics[width=\textwidth]{postmorph.png}
                \caption{With morphological operations (two erodes and two dilates).}
                \label{fig:postmorph}
        \end{subfigure}
        \caption{The same fiducial, both before and after applying morphological operations. Note how the large square is now a continuous contour that can be easily processed in later steps.}
        \label{fig:morph}
\end{figure}

Then, we can use the \emph{dilatation} operator to amplify existing whitespace (which after erosion, should only be the desired fiducial). This works very similarly to erosion, except we take the maximum of the rectangular area rather than the minimum. 

In the actual implementation, we use two erosions with a 3px by 3px kernel, followed by two dilations with the same kernel size.  An example before and after image of the results of applying morphological operations are shown in Fig. \ref{fig:morph}.

At this point, we should have several ``blobs'' of whitespace, with the largest corresponding to the actual fiducial. To pick out the largest blob, we use the OpenCV implementation of the contour finding algorithm described in \cite{Suzuki:85}, which produces a list of detected contours. 

We then use the OpenCV function \mcode{moments} to calculate the area of each contour, and pick out the one  with the largest area (and thus most likely to correspond to the fidiucial). Finally, we take the center point of this contour as the location of our fiducial. We will call the two-dimensional coordinates of this point in the image $p = [u,v]^T$.

\subsection{3D Reconstruction}
\label{sec:3d}
\subsubsection{Reconstruction Summary}
We now wish to estimate where the end of the boom is in 3D space based on our knowledge of where the end of the boom is projected into the 2D image ($p$). Our camera matrix $M \in \real{3 \times 4}$ gives us an unambiguous mapping from the world frame to image space, but we are one degree of freedom short from the other direction. However, we can use known geometry of the boat to add an additional constraint and map unambiguously from the image space to the world frame. 

\subsubsection{Reconstruction Details}

We know the length of the boom and the angle at which it sits relative to the world reference frame ($\alpha$, in Fig. \ref{fig:pram}), and if we assume that the boom rotates about the mast as a perfect circular joint, we know that the tail of the boom (our target point) \emph{must} be constrained to a circle of radius $R = \ell_B\cos{\alpha}$ at height $\hat{y} = \ell_B\sin\alpha$ (see Fig. \ref{fig:overhead}). This means that
\begin{gather}
P_x^2 + P_z^2 = \ell_B^2\cos^2{\alpha}\label{eq:planar}\\
P_y = \ell_B\sin\alpha = \hat{y}\label{eq:py}.
\end{gather}


\begin{figure}[htbp]
   \centering
   \includegraphics[width=.4\textwidth]{overhead} % requires the graphicx package
   \caption{Overhead view of sailboat geometry. Note that the end of the boom is constrained to a circle of radius $R$, and only points between $\pm 90^\circ$ are considered valid (shown in green).}
   \label{fig:overhead}
\end{figure}

We will refer to \eq{planar} as the \emph{radial constraint}. Unfortunately this nonlinear equation is not easily implemented directly as a matrix equation, but we can use the height constraint as an extra constraint. So, we add this knowledge of the $y$-axis coordinate to $M$:
\begin{equation}
\left[\begin{array}{c}p_u \\p_v \\1 \\ \hat{y} \end{array}\right]
= \left[\begin{array}{c}M \\e_2^T\end{array}\right]
\left[\begin{array}{c}P_x \\P_y \\P_z \\1\end{array}\right]
\end{equation}
where 
\[
e_2 = \left[\begin{array}{c}0 \\1 \\0 \\0\end{array}\right].
\]
If we let
\[
A = \left[\begin{array}{c}M \\e_2^T\end{array}\right], 
\]
we can now solve for the position of the target in the real world, in homogenous coordinates:
\[
\left[\begin{array}{c}P_x \\P_y \\P_z \\1\end{array}\right]=A^{-1}
\left[\begin{array}{c}p_u \\p_v \\1\\ \hat{y}\end{array}\right].
\]
Finally, we can extract the position of the end of the boom 
\[
P = \left[\begin{array}{c}P_x \\ P_y \\ P_z\end{array}\right].
\]

Now, we will enforce our planar constraint from \eq{planar} and ``snap" the point onto the allowable circle. We can calculate the closest point on the allowable circle to our target point by taking the vector distance in 3D between the target and the circle center, $O_C = [0, \hat{y}, 0]^T$: 
\[
d = P - O_C = \left[\begin{array}{c}P_x \\P_y \\P_z \end{array}\right] - \left[\begin{array}{c}0 \\ \hat{y} \\ 0\end{array}\right] = \left[\begin{array}{c}P_x\\ 0 \\ P_z\end{array}\right].
\]
We find the unit vector pointing in the direction of $d$, then multiply it by the allowed radius $R$, and add it back to the circle center to get the ``fixed" point $P^\star$:
\[
P^\star = \frac{R}{\norm{d}}d + O_C.
\]
The point $P^\star$ is the point which satisfies the geometrical constraints of the boat in \eq{planar} and \eq{py} that is closest to the point where we believe the end of the boom to be in 3D space.

Now, calculating the boom angle is a simple matter of finding the angle of $d$ about the y-axis:
\[
\phi \approx \tan^{-1}\left({P^\star_x/P^\star_z}\right).
\]
Note that this needs to be the four-quadrant inverse tangent, as implemented in MATLAB's \mcode{atan2.m}.

%\begin{figure}[htbp]
%   \centering
%   \includegraphics[width=.47\textwidth]{sail_overlay} % requires the graphicx package
%   \caption{Calculating the current sail angle. The blue dot is the detected point of the end of the boom in the image, and the yellow x is the constrained position reprojected to the image. The yellow line is all possible end-points projected back into the image. Here the angle was calculated to be $\theta = -60 ^\circ$.}
%   \label{fig:t1}
%\end{figure}

\subsection{Optimal Sail Angle}
\label{sec:optimal}
Rather than try to derive optimal sail angles based on an analytical dynamics approach, I opted to design a heuristic model based on personal experience and widely accepted ``rules of thumb''. 

%\begin{figure}[htbp]
%  \centering
%   \includegraphics[width=.45\textwidth]{boat_frame} 
%  \caption{}
%\end{figure}

Using the reference frames for boat heading and sail angle relative to the boat (see Figure \ref{fig:pos}), I fit a fourth-order polynomial to the data shown in the table below.
\[
\begin{tabular}{c|c|c}
POS & $\theta$ & $\phi$ \\\hline 
Close Hauled & $\pi/4$ & 0 \\\hline 
Beam Reach & $\pi/2$ & $\pi/4$ \\\hline 
Broad Reach & $3\pi/4$ & $\pi/3$\\\hline 
Running & $\pi$ & $\pi/2$\end{tabular}
\]
Using a least-squares fit, I found a polynomial of 
\[
\phi^\star = 0.2702 \theta^3 -1.6977 \theta^2 + 3.833 \theta  -2.0944 
\]
which is shown in Figure \ref{fig:optimal}. I have no doubt that a more accurate model could be developed, but since this is not the focus of the project, I will leave the model as is- it can always be tweaked later. 
\begin{figure}[htbp]
   \centering
   \includegraphics[width=.45\textwidth]{optimal_angle} % requires the graphicx package
   \caption{Heuristic model for optimal sail angle with third-order polynomial fit. This calibration is not intended to be perfect.} 
   \label{fig:optimal}
\end{figure}

%%%%%%%%%%%%%% RESULTS%%%%%%%%%%%%%%%%%%
\section{Experimental Results}
\subsection{Scale Model}
\label{sec:scale}
When this project was still in the early stages, experiments were done on a scale sailboat model, designed explicitly for this application. The scale model featured a $10"\times 10"$ base, with a  10" in mast. A ``boom" was placed around the mast with a roller-ball bearing enclosure, so it could freely swing around the mast. 
\begin{figure}[htbp]
  \centering
   \includegraphics[width=.45\textwidth]{scale_model} 
  \caption{The scale sailboat model designed for rapid prototyping. It has a freely swinging boom and independent wind vane, which could be used for later work.}
   \label{fig:scale}
\end{figure}

\begin{figure}[htbp]
  \centering
   \includegraphics[width=.45\textwidth]{scale_fid} 
  \caption{Detecting the fiducial on the scale model. Here, a teal ping-ping ball was used as the fiducial marker, and the position in the image was easily extracted in real-time. The left shows the RGB image, and the right the thresholded mask. A boom estimate is superimposed in blue.}
   \label{fig:scale_fid}
\end{figure}

The boom also had an attachment at the end for a generic fiducial. A picture of the scale model is shown in Fig. \ref{fig:scale}; it was designed for rapid prototyping in SolidWorks, and then laser-cut out of $1/8$" Duron. 

Unfortunately, because of its small size, implementing the algorithm above was not very successful. While it was no problem detecting a fiducial based on color in the image (see Fig. \ref{fig:scale_fid}), reconsutrcting the sail angle proved difficult. This was due to the fact that the small size of the model introduced many errors during calibration, making the results unusably inaccurate. Fortunately, implementing the algorithm on the full-size boat was much more successful. 
\subsection{Full Size Model}
Implementing the calibration procedure discussed in Sec.~\ref{sec:calibration}, we find a lumped intrinsic/extrinsic camera calibration matrix of
\[
M = \left[\begin{array}{cccc}
8.6593 & -1.5825 & 0.50863 & 735.29 \\
-0.62977 & -10.159 & 1.5544 & 527.47 \\
0 & 0 & 0& 1   \end{array}\right].
\]
Based on boat geometry, we know the boom angle $\alpha=~15.5^\circ$, and the length of the boom $\ell_B = 83$ in. 

\begin{figure*}[htbp]
        \centering
         \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{closeups/t1_overlay}
                \caption{$\phi \approx -58.8^\circ$, $\Delta \phi = 79.9^\circ$}
                \label{fig:t1}
        \end{subfigure}
        ~
        \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{closeups/t7_overlay}
                \caption{$\phi \approx -15.2^\circ$, $\Delta \phi = 36.3^\circ$}
                \label{fig:t2}
        \end{subfigure}
        ~
        \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{closeups/t3_overlay}
                \caption{$\phi \approx 1.6^\circ$, $\Delta \phi = 19.5^\circ$}
                \label{fig:t3}
        \end{subfigure}
		\\
		\begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{closeups/t4_overlay}
                \caption{$\phi \approx 34.5^\circ$,$\Delta \phi = -13.3^\circ$}
                \label{fig:t4}
        \end{subfigure}
        ~
        \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{closeups/t5_overlay}
                \caption{$\phi \approx 22.5^\circ$,$\Delta \phi = -1.4^\circ$}
                \label{fig:t5}
        \end{subfigure}
        ~
        \begin{subfigure}[b]{0.3\textwidth}
                \includegraphics[width=\textwidth]{closeups/t6_overlay}
                \caption{$\phi \approx 8.6^\circ$,$\Delta \phi = 12.4^\circ$}
                \label{fig:t6}
        \end{subfigure}
        
        \caption{Six tests taken with a different boom angle. The detected fiducial point is a blue circle and  the fixed 3D point $P^\star$ is reprojected into the image as a red $\times$. The yellow circle is the possible path of the boom. The calculated boom angle $\phi$ is shown, as well as the suggested trim $\Delta \phi$ based on a heading $\theta = 60^\circ$ (so the optimal sail angle is $\phi^\star = 21.1^\circ$).}
        \label{fig:results}
\end{figure*}

Via manual calibration we find that the color of the yellow fiducial can be well described with the limits
\begin{gather*}
23 \leq H \leq 34\\
46 \leq S \leq 187\\
130 \leq V \leq 256
\end{gather*} 
where each $H,S,V \in [0,255]$.
With this information, we can detect the fiducial position, and map it to the corresponding boom angle. In each figure, the detected fiducial point is shown as a blue circle, and the geometrically compliant point in $P^\star \in \real{3}$ is reprojected into the image, and drawn as a red $\times$. Additionally, a circle representing the radial constraint in \eq{planar} is projected into the image and drawn as a yellow line. It follows that the red $\times$ representing $P^\star$ will always be on the yellow constraining line. Finally, we superimpose a red line representing the boom, which is drawn from the world origin to $P^\star$, then projected into the image. 

\begin{figure}[htbp]

 \centering
         \begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[width=\textwidth]{t3_thresh_rgb}
                \caption{The RGB frame, with overlaid detected fiducial point and image coordinates.}
                \label{fig:t3_thresh_rgb}
        \end{subfigure}
        ~
        \begin{subfigure}[b]{0.23\textwidth}
                \includegraphics[width=\textwidth]{t3_thresh_mask}
                \caption{The corresponding thresholded image, after morphological adjustments.}
                \label{fig:t3_thresh_mask}
        \end{subfigure}
  \caption{Example of OpenCV output looking for the fiducial based on color. Images cropped for emphasis.}
   \label{fig:t3_thresh}
\end{figure}

Figure \ref{fig:results} shows the boom at a six different angles. For these tests, the image capturing and HSV thresholding was done in real-time in OpenCV (see Figure \ref{fig:t3_thresh} for an example), and the 3D reconstruction  was done offline in MATLAB, although this could have also been implemented for real-time use in OpenCV.

Comparing the calculated angles to the actual angles is much harder than it sounds; unfortunately it is very difficult to determine the actual angle of the boom. However, in practice, it does not really matter if the sail angle is exactly where it should be; usually a buffer of $\pm 5^\circ$ is sufficient for optimal performance. Fortunately, the results of this method yield results which appear to be within no more than a $\pm 10 ^\circ$  buffer from their actual values, suggesting that our method was relatively successful.

\section{Conclusion}
As mentioned in the previous section, an error of $\pm 10^\circ$ for estimating the sail angle is actually very good in context, since in practice, the sail angle only needs to be determined to about a $\pm 5^\circ$ tolerance. A reason for the larger margin in the experimental results is likely due to the relatively large size of the fiducial; a smaller fiducial would be able allow the system to better estimate the end-point of the boom in 3D space. However, a smaller fiducial would also make it more difficult to distinguish it from the noise. Additionally, significant changes in lighting over the course of a day may impede attempts to detect a particular color fiducial. An alternative would be to use pattern based fiducial markers instead of colors, which may be more robust to lighting changes. 

The method proposed here only implements detecting the sail angle in real-time, which is sufficient for logging purposes. However, the method of fiducial detection and 3D reconstruction based on geometry could be extended to consider a wind-vane mounted on the boat, which could be similarly processed in real-time to calculate the current heading of the boat. Future work could involve implementing wind direction detection to allow real-time optimal sail angle calculation, which could be compared to the detected current sail angle for suggested adjustments or even a full closed-loop sail angle controller (using a scheme such as that shown in Fig. \ref{fig:closed}). 

\begin{figure}[htbp]
  \centering
   \includegraphics[width=.5\textwidth]{closed-loop} 
  \caption{Proposed closed loop model. }
   \label{fig:closed}
\end{figure}

Data collection for these proof of concept experiments was done on dry-land, and 3D reconstruction was done offline on a laptop computer. However, it would be possible to port the code to a more mobile platform (such as a Raspberry Pi), and actually have the full system running in real-time, on the water. Unfortunately, attempts to compile the OpenCV code for the Raspberry Pi system proved difficult, so such a field test was not able to be done for this project. Future work could further explore the possibility of using OpenCV on a portable system such as a Raspberry Pi, and would have to seriously consider whether such a platform would have the computational resources necessary to run such an algorithm in real-time. 

Finally, it worth drawing attention to the fact that a computer vision based system is not necessary the best way to track the sail angle in real-time; a mechanical solution would likely be more accurate and reliabile. However, this project was chosen out of interest and curiosity, more than practicality, so such ``simple" solutions were not considered. 

\bibliography{./bibcite}{}
\bibliographystyle{plain}


\appendix
The code used for this project is available as a Git repository at \url{http://github.com/apkessler/sailing-coach}. 
\subsection{Real-Time OpenCV Interface (C++)}
The real-time OpenCV image-capture interface in \verb=Code/SailingCoach= was written in C++, and designed to track the a fiducial location in real-time from a camera feed. Running this program allows a user to experiment with different HSV thresholding values, then save/load them to an XML file for later use. This can be done with a source of a live feed, or on preexisting sample images. It also contains a mode for calibrating the camera using a auto-checkerboard detection, although this method (adapted from the OpenCV documentation examples) was not used in this project (calibration was done manually, in MATLAB). 

The tracking code in \mcode{ColorSegmentation.cpp} performs the HSV thresholding and morphological operation in real-time. This code was adapted from (insert link here).It can output the 2D coordinates of the detected fiducial center point, but does not do any 3D reconstruction to estimate the sail angle.

\subsection{3D Reconstruction Code (MATLAB)}
The 3D reconstruction code was implemented in MATLAB to encourage rapid prototyping and debugging - there is no reason why this code could not later be incorporated directly into the real-time C++ interface.
The file \mcode{vision_calibration.m} takes the known world coordinates of calibration points, and uses the corresponding points in the image to estimate the camera matrix $M$, as in Section \ref{sec:calibration}.

The function \mcode{imagePoint2BoomAngle.m} takes a the 2D coordinates of the fiducial in the image, the matrix $M$, and the boat geometry as input and calculates the estimated 3D position of the fiducial point using the algorithm described in Section \ref{sec:3d}. The functions returns the calculated sail angle $\phi$, and can also plot these points, as well as boom reconstruction, on top of the original image. 

Finally, the script \mcode{runOnData.m} iterates over a set of captured frames, and calls \mcode{imagePoint2BoomAngle.m}  to estimate the sail angle for each one, and saves these figures to individual files.
\subsection{Scale Model Files}
The \verb=CAD= subdirectory contains the SolidWorks parts that were used to design and produce the scale model, as discussed in Section \ref{sec:scale}. The vector drawing contained in \verb=laser_cut_file.ai= could be used to quickly reproduce the two-dimensional components on a standard laser-cutter. 
\end{document} 